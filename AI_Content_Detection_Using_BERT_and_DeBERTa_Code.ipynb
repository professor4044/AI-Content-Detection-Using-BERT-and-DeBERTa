{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Load Python Libraries"
      ],
      "metadata": {
        "id": "OlBJQ2DTgzlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "!pip install evaluate\n",
        "import evaluate\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"All libraries loaded successfully.\")"
      ],
      "metadata": {
        "id": "4QYaWdvZg5vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load DataSet"
      ],
      "metadata": {
        "id": "bj_jgGzrioCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_path = 'Training_Essay_Data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "import re\n",
        "def clean_text(text):\n",
        "    if pd.isna(text): return \"\"\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "df = df[df['text'] != \"\"]\n",
        "\n",
        "full_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "train_test_split_dataset = full_dataset.train_test_split(test_size=0.3, seed=42)\n",
        "\n",
        "remaining_dataset = train_test_split_dataset['test'].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_test_split_dataset['train'],\n",
        "    'valid': remaining_dataset['train'],\n",
        "    'test': remaining_dataset['test']\n",
        "})\n",
        "\n",
        "print(\"Dataset Split: 70% Train, 15% Valid, 15% Test - Done.\")\n",
        "print(dataset_dict)"
      ],
      "metadata": {
        "id": "FyKMJ8GMjt9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "P3elRjiwkNsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_checkpoint = \"bert-base-uncased\"\n",
        "deberta_checkpoint = \"microsoft/deberta-v3-small\"\n",
        "\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(bert_checkpoint)\n",
        "tokenizer_deberta = AutoTokenizer.from_pretrained(deberta_checkpoint)\n",
        "\n",
        "def preprocess_bert(examples):\n",
        "    return tokenizer_bert(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
        "\n",
        "def preprocess_deberta(examples):\n",
        "    return tokenizer_deberta(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
        "print(\"Tokenizing for BERT...\")\n",
        "tokenized_bert = dataset_dict.map(preprocess_bert, batched=True)\n",
        "print(\"Tokenizing for DeBERTa...\")\n",
        "tokenized_deberta = dataset_dict.map(preprocess_deberta, batched=True)\n",
        "\n",
        "tokenized_bert = tokenized_bert.remove_columns([\"text\"])\n",
        "tokenized_deberta = tokenized_deberta.remove_columns([\"text\"])\n",
        "print(\"Tokenizations are ready.\")"
      ],
      "metadata": {
        "id": "yGCalW7ekOXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_bert(examples):\n",
        "    # Text-ke token-e rupantor\n",
        "    result = tokenizer_bert(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
        "\n",
        "    result[\"labels\"] = examples[\"generated\"]\n",
        "    return result\n",
        "\n",
        "print(\"Tokenizing with labels, please wait...\")\n",
        "tokenized_bert = dataset_dict.map(preprocess_bert, batched=True)\n",
        "\n",
        "tokenized_bert = tokenized_bert.remove_columns([\"text\", \"generated\"])\n",
        "\n",
        "print(\"Now the dataset is ready.\")"
      ],
      "metadata": {
        "id": "aXVh0HuouxeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT"
      ],
      "metadata": {
        "id": "oGdaSlU-oHum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "id2label = {0: \"HUMAN\", 1: \"AI\"}\n",
        "label2id = {\"HUMAN\": 0, \"AI\": 1}\n",
        "\n",
        "model_bert = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "print(\"BERT Model Setup Completed.\")"
      ],
      "metadata": {
        "id": "E2gwK1XSoJnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT Training Arguments"
      ],
      "metadata": {
        "id": "NjOB7Y7Xo1EA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer_bert)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert-ai-detection\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer_bert = Trainer(\n",
        "    model=model_bert,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_bert[\"train\"],\n",
        "    eval_dataset=tokenized_bert[\"valid\"],\n",
        "    tokenizer=tokenizer_bert,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Training Arguments setup ready.\")"
      ],
      "metadata": {
        "id": "e2o1Rrz7p4Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_bert.train()"
      ],
      "metadata": {
        "id": "zqWyO_MZqA9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"calculating metrics for BERT on Test Set...\")\n",
        "bert_results = trainer_bert.evaluate(tokenized_bert[\"test\"])\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"BERT PERFORMANCE REPORT\")\n",
        "print(\"=\"*30)\n",
        "print(f\"Accuracy  : {bert_results['eval_accuracy']:.4f}\")\n",
        "print(f\"Precision : {bert_results['eval_precision']:.4f}\")\n",
        "print(f\"Recall    : {bert_results['eval_recall']:.4f}\")\n",
        "print(f\"F1 Score  : {bert_results['eval_f1']:.4f}\")\n",
        "print(\"=\"*30)"
      ],
      "metadata": {
        "id": "o67jyYbnfLSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeBERTa"
      ],
      "metadata": {
        "id": "kXhq1xxVfgai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deberta_checkpoint = \"microsoft/deberta-v3-small\"\n",
        "tokenizer_deberta = AutoTokenizer.from_pretrained(deberta_checkpoint)\n",
        "\n",
        "def preprocess_deberta(examples):\n",
        "    result = tokenizer_deberta(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
        "    result[\"labels\"] = examples[\"generated\"]\n",
        "    return result\n",
        "\n",
        "print(\"Tokenizing for DeBERTa with labels...\")\n",
        "tokenized_deberta = dataset_dict.map(preprocess_deberta, batched=True)\n",
        "\n",
        "tokenized_deberta = tokenized_deberta.remove_columns([\"text\", \"generated\"])\n",
        "\n",
        "print(\"DeBERTa Tokenization ready with labels.\")"
      ],
      "metadata": {
        "id": "2jwgNidvfiaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeBERTa Training Arguments"
      ],
      "metadata": {
        "id": "kCg5bHnCfNNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_deberta = AutoModelForSequenceClassification.from_pretrained(\n",
        "    deberta_checkpoint,\n",
        "    num_labels=2,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "data_collator_deberta = DataCollatorWithPadding(tokenizer=tokenizer_deberta)\n",
        "\n",
        "training_args_deberta = TrainingArguments(\n",
        "    output_dir=\"./deberta-ai-detection\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps=100,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer_deberta = Trainer(\n",
        "    model=model_deberta,\n",
        "    args=training_args_deberta,\n",
        "    train_dataset=tokenized_deberta[\"train\"],\n",
        "    eval_dataset=tokenized_deberta[\"valid\"],\n",
        "    tokenizer=tokenizer_deberta,\n",
        "    data_collator=data_collator_deberta,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"DeBERTa Setup ready.\")"
      ],
      "metadata": {
        "id": "7ZqD4mkRlOVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_deberta.train()"
      ],
      "metadata": {
        "id": "fDC5xWxqlUvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Calculating metrics for DeBERTa on Test Set...\")\n",
        "deberta_results = trainer_deberta.evaluate(tokenized_deberta[\"test\"])\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"DeBERTa PERFORMANCE REPORT\")\n",
        "print(\"=\"*30)\n",
        "print(f\"Accuracy  : {deberta_results['eval_accuracy']:.4f}\")\n",
        "print(f\"Precision : {deberta_results['eval_precision']:.4f}\")\n",
        "print(f\"Recall    : {deberta_results['eval_recall']:.4f}\")\n",
        "print(f\"F1 Score  : {deberta_results['eval_f1']:.4f}\")\n",
        "print(\"=\"*30)"
      ],
      "metadata": {
        "id": "QQLS4BmRtgGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison"
      ],
      "metadata": {
        "id": "pIhi_W2rusC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "summary = {\n",
        "    \"AI_Detection\": {\n",
        "        \"bert\": {\n",
        "            \"precision\": bert_results['eval_precision'],\n",
        "            \"recall\": bert_results['eval_recall'],\n",
        "            \"f1\": bert_results['eval_f1'],\n",
        "            \"accuracy\": bert_results['eval_accuracy']\n",
        "        },\n",
        "        \"deberta\": {\n",
        "            \"precision\": deberta_results['eval_precision'],\n",
        "            \"recall\": deberta_results['eval_recall'],\n",
        "            \"f1\": deberta_results['eval_f1'],\n",
        "            \"accuracy\": deberta_results['eval_accuracy']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "def get_predictions(trainer, tokenized_dataset):\n",
        "    preds = trainer.predict(tokenized_dataset)\n",
        "    y_pred = np.argmax(preds.predictions, axis=-1)\n",
        "    y_true = preds.label_ids\n",
        "    return y_true, y_pred\n",
        "\n",
        "print(\"Fetching predictions for Confusion Matrix...\")\n",
        "y_true_bert, y_pred_bert = get_predictions(trainer_bert, tokenized_bert[\"test\"])\n",
        "y_true_deb, y_pred_deb = get_predictions(trainer_deberta, tokenized_deberta[\"test\"])\n",
        "\n",
        "def plot_confusion(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Human', 'AI'], yticklabels=['Human', 'AI'])\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n--- Confusion Matrix: BERT ---\")\n",
        "plot_confusion(y_true_bert, y_pred_bert, \"BERT (AI Detection - Test Set)\")\n",
        "\n",
        "print(\"\\n--- Confusion Matrix: DeBERTa ---\")\n",
        "plot_confusion(y_true_deb, y_pred_deb, \"DeBERTa (AI Detection - Test Set)\")\n",
        "\n",
        "metrics = [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
        "model_names = [\"BERT\", \"DeBERTa\"]\n",
        "\n",
        "for metric in metrics:\n",
        "    plt.figure(figsize=(6,4))\n",
        "\n",
        "    values = [summary[\"AI_Detection\"][\"bert\"][metric],\n",
        "              summary[\"AI_Detection\"][\"deberta\"][metric]]\n",
        "\n",
        "    colors = ['skyblue', 'salmon']\n",
        "    plt.bar(model_names, values, color=colors, width=0.5)\n",
        "\n",
        "    for i, v in enumerate(values):\n",
        "        plt.text(i, v + 0.01, f\"{v:.4f}\", ha='center', fontweight='bold')\n",
        "\n",
        "    plt.ylim(0, 1.1)\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.title(f\"Model Comparison: {metric.capitalize()}\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BcuNVweAuumf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}